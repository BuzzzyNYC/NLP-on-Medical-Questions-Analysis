{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bb0f6b7",
   "metadata": {},
   "source": [
    "# Analyzing Medical Questions using Natural Language Processing\n",
    "\n",
    "This notebook is to analyze questions asked by health care providers ('providers') and the general public ('patient') so as to glean insight from both the provider and patient viewpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffbd39c",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "At this point, questions have been manually spell-checked for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829928d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read datasets \n",
    "path = 'data/'  \n",
    "filePrefix = ''\n",
    "categories = ['patients','providers']\n",
    "dataset = {}\n",
    "dataset_raw = {}\n",
    "allFeatures = set()\n",
    "questions = 0\n",
    "question_count = {}\n",
    "\n",
    "corpus = []\n",
    "text = \"\"\n",
    "\n",
    "N={} # Number of questions in each corpus\n",
    "\n",
    "for category in categories:\n",
    "    fileName = path + filePrefix + category.lower() + '.txt'\n",
    "    f = open(fileName,'r')\n",
    "    text = ''\n",
    "    text_raw = ''    \n",
    "    lines = f.readlines()\n",
    "    questions += len(lines)\n",
    "    question_count[category] = len(lines)\n",
    "    dataset_raw[category] = list(map(lambda line: line.lower(), lines))\n",
    "    \n",
    "    for line in lines:\n",
    "        text += line.replace('\\n',' ').lower()\n",
    "        text_raw = line.lower()\n",
    "    f.close\n",
    "    N[category] = len(lines)\n",
    "    \n",
    "    #create tokens\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    dataset[category] = nltk.Text(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b474ad",
   "metadata": {},
   "source": [
    "## Removing Punctuation & Stopwording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "providersFD = FreqDist(dataset['providers'])\n",
    "patientsFD = FreqDist(dataset['patients'])\n",
    "\n",
    "punctuations = \".,\\\"-\\\\/#!?$%\\^&\\*;:{}=\\-_'~()\"\n",
    "print ('Punctuation FD[providers] FD[patients]')\n",
    "for punct in punctuations:\n",
    "    print ('   {}  {:3d}   {:3d}'.format(punct,providersFD[punct], patientsFD[punct]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d431e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "dsCleaned = {} #cleaned dataset\n",
    "\n",
    "def removePunctuation(corpus):\n",
    "    punctuations = \".,\\\"-\\\\/#!?$%\\^&\\*;:{}=\\-_'~()\"    \n",
    "    filteredCorpus = [token for token in corpus if (not token in punctuations)]\n",
    "    return filteredCorpus\n",
    "\n",
    "def stopwording(corpus, min_len):\n",
    "#     black_list = ['patients','providers']\n",
    "    filteredCorpus = [token for token in corpus if (not token in stopwords.words('english') and len(token) > min_len)]\n",
    "    return filteredCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edd5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punct & apply stopwording\n",
    "for category in categories:\n",
    "    print ('Processing %s' % category)\n",
    "    dsCleaned[category] = stopwording(removePunctuation(dataset[category]), 3)\n",
    "    print (dsCleaned[category])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202a2ba",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "dsFinal={} #working dataset (final)\n",
    "\n",
    "# def stemming(corpus):\n",
    "#     stemmer = nltk.PorterStemmer()\n",
    "#     normalized_corpus = [stemmer.stem(token) for token in corpus]\n",
    "#     return normalized_corpus\n",
    "\n",
    "def lemmatization(corpus):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    normalized_corpus = [lemmatizer.lemmatize(token) for token in corpus]\n",
    "    return normalized_corpus\n",
    "\n",
    "for category in categories:\n",
    "    print ('Processing %s' % category)\n",
    "    dsFinal[category] = lemmatization(dsCleaned[category])\n",
    "    print (dsFinal[category])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693b279",
   "metadata": {},
   "source": [
    "## Simple Analysis\n",
    "### Get Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Patient questions have a total of %s tokens and a vocabulary size of %s' % (len(dsFinal['patients']), len(vocabulary['patients'])))\n",
    "print ('Provider questions have a total of %s tokens and a vocabulary size of %s' % (len(dsFinal['providers']), len(vocabulary['providers'])))\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(text)*1.0/len(set(text))\n",
    "\n",
    "lexDiversity = {}\n",
    "for category in categories:\n",
    "    lexDiversity[category] = lexical_diversity(dsFinal[category])\n",
    "    print ('Lexical Diversity in %s = %s' % (category,lexDiversity[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb177eae",
   "metadata": {},
   "source": [
    "### Counting Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d99c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ('# time \"disease\" is used by health care providers %s' % ds['providers'].count('disease'))\n",
    "# print ('# time \"disease\" is used by patients %s' % ds['patients'].count('disease'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f7561",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "\n",
    "# for token in vocabulary[category]:\n",
    "#     count[token] = dsFinal[category].count(token)\n",
    "\n",
    "for token in vocabulary['providers']:\n",
    "    count[token] = dsFinal['providers'].count(token)\n",
    "    \n",
    "for w in sorted(count, key = count.get, reverse=True):\n",
    "    print (w, count[w]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "\n",
    "for token in vocabulary['patients']:\n",
    "    count[token] = dsFinal['patients'].count(token)\n",
    "\n",
    "for w in sorted(count, key = count.get, reverse=True):\n",
    "    print (w, count[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8b7c03",
   "metadata": {},
   "source": [
    "### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc6926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequent(freq, n):\n",
    "    result = {}\n",
    "    index = 0\n",
    "    for i in sorted(freq, key = freq.get, reverse = True):\n",
    "        index += 1\n",
    "        result[i] = freq[i]\n",
    "        if index == n:\n",
    "            break\n",
    "    return result \n",
    "\n",
    "frequency = nltk.FreqDist(dsFinal[category])\n",
    "\n",
    "topTokens = getFrequent(frequency, 50)\n",
    "\n",
    "print(topTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb8c45",
   "metadata": {},
   "source": [
    "### Finding Important Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8544a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "providersFD = FreqDist(dsFinal['providers'])\n",
    "patientsFD = FreqDist(dsFinal['patients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a24479",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FreqDist(dsFinal[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e8cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "providersFD.plot(20, cumulative = False, title = 'Provider Tokens');\n",
    "patientsFD.plot(20, cumulative = False, title = 'Patient Tokens');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in dsFinal['patients']:\n",
    "    if (len(token) >= 8):\n",
    "        print ('%s [%s]' % (token, patientsFD[token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7fe01e",
   "metadata": {},
   "source": [
    "### Collocations, 2-grams & Co-Occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsFinal['patients']\n",
    "dsFinal['patients'] = nltk.Text(dsFinal['patients'])\n",
    "dsFinal['patients'].collocation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef49e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsFinal['providers']\n",
    "dsFinal['providers'] = nltk.Text(dsFinal['providers'])\n",
    "dsFinal['providers'].collocation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c88ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "from nltk.util import ngrams\n",
    "\n",
    "print ('Generating bigrams')\n",
    "bigrams = ngrams(dsFinal['patients'],2)\n",
    "for bigram in bigrams:\n",
    "    print (bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef7685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = nltk.collocations.BigramAssocMeasures()\n",
    "trigram = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Finding frequent 2-grams\n",
    "print ('Finding frequent 2-grams')\n",
    "for category in categories:\n",
    "    finder = BigramCollocationFinder.from_words(dsFinal[category])\n",
    "    finder.apply_freq_filter(5)\n",
    "    tokens = finder.nbest(bigram.pmi, 20)\n",
    "    print (tokens)\n",
    "    \n",
    "# Finding frequent 3-grams\n",
    "print ('\\nFinding frequent 3-grams')\n",
    "for category in categories:\n",
    "    finder = TrigramCollocationFinder.from_words(dsFinal[category])\n",
    "    finder.apply_freq_filter(5)\n",
    "    tokens = finder.nbest(trigram.pmi, 20)\n",
    "    print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfcad0",
   "metadata": {},
   "source": [
    "### Lexical Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fdea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_cleantext(corpus, language):\n",
    "    stopwords = nltk.corpus.stopwords.words(language)\n",
    "    cleantext = [token for token in corpus if token not in stopwords]\n",
    "    return len(cleantext)*1.0/len(corpus)\n",
    "\n",
    "language='english'\n",
    "for category in categories:\n",
    "    print (\"Proportion of clean terms in the [%s] is %s\" % (category,proportion_cleantext(dataset[category],language)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ecb32",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "### Topical Discovery and Latent Dirichlet Allocation\n",
    "Identify the overall topic of discussion\n",
    "\n",
    "Identify topics brought forward by Patients\n",
    "\n",
    "Identify topics brought forward by Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39082711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore depreciation warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.collocations import *\n",
    "from gensim import corpora\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_utf(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r' ', text)\n",
    "\n",
    "def load_stopwords():\n",
    "    swords = []\n",
    "    path=\"data/stopwords.txt\"\n",
    "    file_input = open (path,\"r\")\n",
    "    lines = file_input.readlines()\n",
    "    for line in lines:\n",
    "        swords.append(line[:-1])\n",
    "    file_input.close()\n",
    "    return swords\n",
    "\n",
    "def loadCorpus(path):\n",
    "    data = []\n",
    "    file_input = open (path,\"r\")\n",
    "    lines = file_input.readlines()\n",
    "    for line in lines:\n",
    "        data.append(remove_utf(line[:-1].lower()))\n",
    "    file_input.close()\n",
    "    return data\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "path = \"data/splitcombo.txt\"\n",
    "allQuestions = loadCorpus(path)\n",
    "#print (allQuestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c31481",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsAsked = {'patients:':[],'providers:':[]}\n",
    "\n",
    "keys = questionsAsked.keys()\n",
    "\n",
    "current = \"\"\n",
    "for line in allQuestions:\n",
    "    if len(line)>5:\n",
    "        for key in keys:\n",
    "            if line.startswith(key):\n",
    "                current = key\n",
    "        l = questionsAsked[current]\n",
    "        l.append(line)\n",
    "        questionsAsked[current]=l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ef343",
   "metadata": {},
   "source": [
    "### More preprocessing for LDA topic modeling (builds on the cleaning, stopwording and lemmatization steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f24341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stopwording(corpus, min_len):\n",
    "    black_list = ['patients','providers']\n",
    "    filteredCorpus = [token for token in corpus if (not token in stopwords and not token in black_list and len(token)>min_len)]\n",
    "    return filteredCorpus\n",
    "\n",
    "def getCollocations(text, min_freq, coll_num):\n",
    "    bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(text)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    collocations = finder.nbest(bigrams.pmi, coll_num)\n",
    "    return collocations\n",
    "\n",
    "def replaceCollocationsInText(text,collocations):\n",
    "    first = [t[0]for t in collocations]\n",
    "    second = [t[1] for t in collocations]\n",
    "\n",
    "    dtokens = []\n",
    "    i = 0\n",
    "    while i<=(len(text)-1):\n",
    "        try:\n",
    "            idx1 = first.index(text[i])\n",
    "            if (text[i+1]==second[idx1]):\n",
    "                dtokens.append(first[idx1]+\"_\"+second[idx1])\n",
    "                i=i+1\n",
    "        except:\n",
    "            dtokens.append(text[i])\n",
    "            pass\n",
    "        i=i+1\n",
    "    return dtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc94ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCorpus(corpusData):\n",
    "    min_frequency = 3\n",
    "    num_of_collocations=100\n",
    "    text=\"\"\n",
    "    corpus=[]\n",
    "    token =[]\n",
    "    \n",
    "    #Extract corpus and preprocess data\n",
    "    for line in corpusData:\n",
    "        t = nltk.word_tokenize(line)\n",
    "        doc = nltk.Text(t)\n",
    "        doc_clean = nltk.Text(lemmatization(apply_stopwording(removePunctuation(doc), 3)))\n",
    "        corpus.append(doc_clean)\n",
    "        token.extend(doc_clean.tokens)\n",
    "        text=text+line\n",
    "    \n",
    "    #Identify collocations\n",
    "    collocations = getCollocations(tokens,min_frequency,num_of_collocations)\n",
    "    docs = []\n",
    "    for doc in corpus:\n",
    "        t = replaceCollocationsInText(doc,collocations)\n",
    "        if (len(t)>0):\n",
    "            docs.append(replaceCollocationsInText(doc,collocations))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d998a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = processCorpus(allQuestions)\n",
    "print(len(docs))\n",
    "print (docs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e19d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag-of-words representation of the dictionary\n",
    "k = 10\n",
    "iterations = 40\n",
    "\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "topic_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=k, id2word = dictionary, passes = iterations)\n",
    "lda_vis = pyLDAvis.gensim_models.prepare(topic_model,corpus,dictionary,sort_topics=False)\n",
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ae5f0",
   "metadata": {},
   "source": [
    "## Vizualizing Topics by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eae406",
   "metadata": {},
   "outputs": [],
   "source": [
    "##vizualizing provider topics \n",
    "\n",
    "k = 5\n",
    "\n",
    "docs = processCorpus(questionsAsked[\"providers:\"])\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "topic_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=k, id2word = dictionary, passes = iterations)\n",
    "lda_vis = pyLDAvis.gensim_models.prepare(topic_model,corpus,dictionary,sort_topics=False)\n",
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##vizualizing patient topics\n",
    "\n",
    "k = 4\n",
    "\n",
    "docs = processCorpus(questionsAsked[\"patients:\"])\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "topic_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=k, id2word = dictionary, passes = iterations)\n",
    "lda_vis = pyLDAvis.gensim_models.prepare(topic_model,corpus,dictionary,sort_topics=False)\n",
    "pyLDAvis.display(lda_vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
